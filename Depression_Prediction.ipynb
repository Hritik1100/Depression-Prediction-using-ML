{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hritik1100/Depression-Prediction-using-ML/blob/main/Depression_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Depression Prediction on Twitter using Machine Learning Algorithms**"
      ],
      "metadata": {
        "id": "eg78WNpKrsNo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import data and download required libraries"
      ],
      "metadata": {
        "id": "aMh0qCmQrzXM"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYSJIcByb1kD"
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "import sklearn\n",
        "import spacy\n",
        "data = pd.read_csv('newfile.csv',encoding='latin-1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WO44n7Pv93Zh"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('words')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGnydtmp93bm"
      },
      "source": [
        "from nltk.corpus import stopwords,wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "lem = WordNetLemmatizer()\n",
        "corpus_test = []\n",
        "stopwords = set(stopwords.words('english'))\n",
        "punc = string.punctuation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cf5AZKB093dx"
      },
      "source": [
        "!pip install git+https://github.com/casics/spiral.git\n",
        "!pip install textaugment\n",
        "!pip install contractions\n",
        "!pip install -U git+https://github.com/ray-project/tune-sklearn.git && pip install 'ray[tune]'\n",
        "!!pip install scikit-optimize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oaElyLCX_IOC",
        "outputId": "acc52e78-baa4-4a8e-f131-e3d6d6891963"
      },
      "source": [
        "from textaugment import Wordnet\n",
        "t = Wordnet()\n",
        "aug = t.augment(\"The quick brown fox jumps over the lazy dog\")\n",
        "print(aug)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the quick brown fox skip over the lazy dog\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing"
      ],
      "metadata": {
        "id": "VmDC9KCZr3oc"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1d0Ci0e93w2"
      },
      "source": [
        "corpus=[]\n",
        "import contractions\n",
        "from spiral import ronin\n",
        "for i in range(len(data)):\n",
        "  #review = re.sub(r\"http\\S+\", \"\", data['Tweets'][i])\n",
        "  #review = re.sub('[^a-zA-Z]',' ',review)\n",
        "  review = re.sub(r\"http\\S+\", \"\", data['message'][i]) # remove urls\n",
        "  review = re.sub(r'<([^>]*)>', ' ', review) # remove emojis\n",
        "  review = re.sub(r'@\\w+', ' ', review) # remove at mentions\n",
        "  review = re.sub(r'#', '', review) # remove hashtag symbol\n",
        "  review = re.sub(r'[0-9]+', ' ', review) # remove numbers\n",
        "  review = re.sub(r'[^A-Za-z0-9,?.!]+', ' ', review)\n",
        "  review = review.lower()\n",
        "  l=[]\n",
        "  for word in review.split():\n",
        "    l.append(contractions.fix(word))\n",
        "  review = ' '.join(l)\n",
        "  review = t.augment(review)\n",
        "  review = ronin.split(review)\n",
        "  review = ' '.join(review)\n",
        "  review = nltk.word_tokenize(review)\n",
        "  review = [lem.lemmatize(word) for word in review if word not in stopwords and word not in punc]\n",
        "  review = ' '.join(review)\n",
        "  corpus.append(review)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting data into Train and Test set"
      ],
      "metadata": {
        "id": "QymCOpldr5gO"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09Sp64dRhTBu"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X = corpus\n",
        "y = data['label']\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Engineering (Using Count Vectorizer)"
      ],
      "metadata": {
        "id": "BwDMBtmcr7aK"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wl0Zorv-93y5"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "fun = CountVectorizer(max_features=1500)\n",
        "X_train_vector = fun.fit_transform(X_train).toarray()\n",
        "X_test_vector = fun.transform(X_test).toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyper Parameters"
      ],
      "metadata": {
        "id": "ypNrVGJZr9II"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZuWb2nGPw04_"
      },
      "source": [
        "params = {'clf__max_depth': [200,300],\n",
        "          'clf__max_features': ['sqrt'],\n",
        "          'clf__min_samples_leaf': [2],\n",
        "          'clf__splitter': ['best']}\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Finding the right Hyperparameters using TuneSearchCV"
      ],
      "metadata": {
        "id": "tHbKizjlr_Jv"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "Muun7jTXvFll",
        "outputId": "9f3c3b72-4c13-4c90-84d8-c6fe2097d435"
      },
      "source": [
        "from sklearn.metrics import precision_score,confusion_matrix, precision_recall_curve, auc, roc_auc_score, roc_curve, recall_score, classification_report\n",
        "from tune_sklearn import TuneSearchCV\n",
        "from imblearn.over_sampling import SMOTE           #Run this for Cross Validation after initializing params\n",
        "from imblearn.pipeline import Pipeline, make_pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
        "scoring = {'accuracy': make_scorer(accuracy_score),\n",
        "           'precision': make_scorer(precision_score, average = 'weighted'),\n",
        "           'recall': make_scorer(recall_score, average = 'weighted'),\n",
        "           'f1': make_scorer(f1_score, average = 'weighted')\n",
        "}\n",
        "\n",
        "pipeline = Pipeline([('Smote',SMOTE(random_state=42)),('clf',DecisionTreeClassifier())])\n",
        "tune_imba = TuneSearchCV(pipeline,params,scoring=scoring,max_iters=10,n_jobs=-1,search_optimization = 'bayesian',cv=5,verbose=1,refit='precision')\n",
        "tune_imba.fit(X_train_vector,y_train)\n",
        "y_pred = tune_imba.predict(X_test_vector)\n",
        "print('validation Precision',tune_imba.best_score_)\n",
        "print(\"Test precision score - \", precision_score(y_test, y_pred))\n",
        "print(\"Test recall score - \", recall_score(y_test, y_pred))\n",
        "print(\"Test f1 score - \", f1_score(y_test, y_pred))\n",
        "print(\"Test accuracy score - \", accuracy_score(y_test, y_pred))\n",
        "tune_imba.best_params_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 1.8/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/2 CPUs, 0/0 GPUs, 0.0/7.37 GiB heap, 0.0/2.54 GiB objects<br>Result logdir: /root/ray_results/_Trainable_2021-04-10_17-21-33<br>Number of trials: 10/10 (10 TERMINATED)<br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:ray.tune.tune:Total run time: 385.57 seconds (385.50 seconds for the tuning loop).\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "validation Precision 0.8285910987559312\n",
            "Test precision score -  0.5692599620493358\n",
            "Test recall score -  0.6571741511500547\n",
            "Test f1 score -  0.6100660904931366\n",
            "Test accuracy score -  0.8108508014796547\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'clf__max_depth': 237,\n",
              " 'clf__max_features': 'sqrt',\n",
              " 'clf__min_samples_leaf': 2,\n",
              " 'clf__splitter': 'best'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Employing Gradient Boost ML Model"
      ],
      "metadata": {
        "id": "t1tGHl9PsF9S"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oj4YLVAo-MBQ",
        "outputId": "f5e1b617-335a-488f-af06-288a6a7e7386"
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report, accuracy_score, f1_score\n",
        "\n",
        "def evaluate_model(X_train, y_train, model):\n",
        "    model.fit(X_train, y_train)\n",
        "    preds = model.predict(X_test_vector)\n",
        "    print('Accuracy:', accuracy_score(y_test, preds))\n",
        "    print('F1 score:', f1_score(y_test,preds,average='weighted'))\n",
        "    print('Recall:', recall_score(y_test, preds,average='weighted'))\n",
        "    print('Precision:', precision_score(y_test, preds,average='weighted'))\n",
        "\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "model = GradientBoostingClassifier(n_estimators=100, random_state=10)\n",
        "evaluate_model(X_train_SMOTE,y_train_SMOTE,model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9393341553637484\n",
            "F1 score: 0.9365387923693352\n",
            "Recall: 0.9393341553637484\n",
            "Precision: 0.9410342020495727\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
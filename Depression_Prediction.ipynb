{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hritik1100/Depression-Prediction-using-ML/blob/main/Depression_Prediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eIvaRTc4W0Z7"
      },
      "source": [
        "**Depression Prediction on Twitter using Machine Learning Algorithms**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t1W9iDm1WhY7"
      },
      "source": [
        "Import data and download required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AEmU3HXXVEN"
      },
      "source": [
        "!pip install git+https://github.com/casics/spiral.git  #Spiral\n",
        "!pip install contractions   #Word Contraction\n",
        "!pip install -U git+https://github.com/ray-project/tune-sklearn.git && pip install 'ray[tune]'  #TuneSearch\n",
        "!!pip install scikit-optimize\n",
        "!pip install ConfigSpace"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnMqC5cHcmtE"
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "import sklearn\n",
        "data = pd.read_csv('dataset.csv',encoding='latin-1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QLAUrQ9aczfk"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\n",
        "nltk.download('words')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kM8fpuxuczqd"
      },
      "source": [
        "from nltk.corpus import stopwords,wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "lem = WordNetLemmatizer()\n",
        "corpus_test = []\n",
        "stopwords = set(stopwords.words('english'))\n",
        "punc = string.punctuation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHkgGtYJXPWB"
      },
      "source": [
        "Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-7W2mlFcz1C"
      },
      "source": [
        "corpus=[]\n",
        "import contractions\n",
        "from spiral import ronin\n",
        "for i in range(len(data)):\n",
        "  review = re.sub(r\"http\\S+\", \"\", data['message'][i]) # remove urls\n",
        "  review = re.sub(r'<([^>]*)>', ' ', review) # remove emojis\n",
        "  review = re.sub(r'@\\w+', ' ', review) # remove @ mentions\n",
        "  review = re.sub(r'#', '', review) # remove hashtag symbol\n",
        "  review = re.sub(r'[0-9]+', ' ', review) # remove numbers\n",
        "  review = re.sub(r'[^A-Za-z0-9,?.!]+', ' ', review)\n",
        "  review = review.lower()\n",
        "  l=[]\n",
        "  for word in review.split():\n",
        "    l.append(contractions.fix(word))      #you're ------> you are\n",
        "  review = ' '.join(l)\n",
        "  review = ronin.split(review)            #mentalhealth----> mental health\n",
        "  review = ' '.join(review)\n",
        "  review = nltk.word_tokenize(review)\n",
        "  review = [lem.lemmatize(word) for word in review if word not in stopwords and word not in punc]\n",
        "  review = ' '.join(review)\n",
        "  corpus.append(review)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aO0YRzivXxip"
      },
      "source": [
        "Splitting data into Train and Test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NAVB-W9fcz3A"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X = corpus\n",
        "y = data['label']\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.3,random_state=0) #70:30"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sBhs3EbwX5nZ"
      },
      "source": [
        "Feature Engineering (Using Count Vectorizer)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcLupAR4cz5A"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer    #Count Vectorizer\n",
        "fun = CountVectorizer(max_features=1500)\n",
        "X_train_vector = fun.fit_transform(X_train).toarray()\n",
        "X_test_vector = fun.transform(X_test).toarray()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQ1I8yLVYCwL"
      },
      "source": [
        "Hyper Parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QkiSC3ExQfE_"
      },
      "source": [
        "params = {'clf__learning_rate':[0.2,0.3],  #Gradient Boost Hyperparameters\n",
        "          'clf__n_estimators':[150,250]}\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2H8WQklWTkdH"
      },
      "source": [
        "Finding the right Hyperparameters using TuneSearchCV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "zcGi7vi4QDCV",
        "outputId": "cd364502-b9ce-4a52-ff90-8ba29c918bf2"
      },
      "source": [
        "from sklearn.metrics import precision_score,confusion_matrix, precision_recall_curve, auc, roc_auc_score, roc_curve, recall_score, classification_report\n",
        "from tune_sklearn import TuneSearchCV\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline, make_pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "scoring = {'accuracy': make_scorer(accuracy_score),\n",
        "           'precision': make_scorer(precision_score, average = 'weighted'),\n",
        "           'recall': make_scorer(recall_score, average = 'weighted'),\n",
        "           'f1': make_scorer(f1_score, average = 'weighted')\n",
        "}\n",
        "\n",
        "pipeline = Pipeline([('Smote',SMOTE(random_state=42)),('clf',GradientBoostingClassifier())])\n",
        "\n",
        "##5 fold Cross Validation\n",
        "tune_imba = TuneSearchCV(pipeline,params,scoring=scoring,max_iters=10,n_jobs=-1,search_optimization = 'bayesian',cv=5,verbose=1,refit='f1')\n",
        "\n",
        "tune_imba.fit(X_train_vector,y_train)\n",
        "y_pred = tune_imba.predict(X_test_vector)\n",
        "print('Validation F1-Score',tune_imba.best_score_)\n",
        "print(\"Test F1-score - \", f1_score(y_test, y_pred))\n",
        "print(\"Test precision score - \", precision_score(y_test, y_pred))\n",
        "print(\"Test recall score - \", recall_score(y_test, y_pred))\n",
        "print(\"Test accuracy score - \", accuracy_score(y_test, y_pred))\n",
        "tune_imba.best_params_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "== Status ==<br>Memory usage on this node: 1.7/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 0/2 CPUs, 0/0 GPUs, 0.0/7.34 GiB heap, 0.0/3.67 GiB objects<br>Result logdir: /root/ray_results/_Trainable_2021-05-27_13-37-32<br>Number of trials: 10/10 (10 TERMINATED)<br><br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:ray.tune.tune:Total run time: 6639.06 seconds (6636.05 seconds for the tuning loop).\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function safe_indexing is deprecated; safe_indexing is deprecated in version 0.22 and will be removed in version 0.24.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation F1-Score 0.9328491177116709\n",
            "Test F1-score -  0.8480189237137788\n",
            "Test precision score -  0.9215938303341902\n",
            "Test recall score -  0.7853231106243155\n",
            "Test accuracy score -  0.9366214549938348\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'clf__learning_rate': 0.23683576691451325, 'clf__n_estimators': 186}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    }
  ]
}